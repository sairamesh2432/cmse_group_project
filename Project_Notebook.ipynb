{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_data = pd.read_csv(\"AmesHousing.csv\")\n",
    "#only looking at numerical data for now to make things simple\n",
    "numerical_columns = ames_data.describe().columns\n",
    "numerical_ames_data = ames_data[numerical_columns]\n",
    "numerical_ames_data = numerical_ames_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next three cells are just preprocessing stuff I'm playing around with, not a big deal for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_na_mask = ames_data.isna().sum()\n",
    "missing_columns = is_na_mask[is_na_mask > 1000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_data.drop(columns = missing_columns, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = numerical_columns[:len(numerical_columns)-1]\n",
    "feature_vectors = np.array(numerical_ames_data[feature_columns])\n",
    "class_labels = np.array(numerical_ames_data[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used a univariate selection for the feature extraction step:\n",
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SelectKBest(score_func=f_regression, k=15)\n",
    "fit = test.fit(feature_vectors, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = fit.transform(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_best_features = list(numerical_ames_data.columns[fit.get_support(indices=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just to be sure of the best features, we also want to use a random forest method to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial split\n",
    "total_x_train, total_x_test, total_y_train, total_y_test = train_test_split(feature_vectors, class_labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_select_regr = RandomForestRegressor(n_estimators = 100, random_state=0)\n",
    "feat_select_regr.fit(total_x_train, total_y_train)\n",
    "important_features = list(zip(numerical_columns,feat_select_regr.feature_importances_))\n",
    "important_features.sort(key = lambda pair: pair[1], reverse = True)\n",
    "random_f_best_features = [pair[0] for pair in important_features[0:15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the intersection between the features selected from the random forest method and the univariate selection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_best_features = list(set(univariate_best_features) & set(random_f_best_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, onto the Random Forest Prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_df = numerical_ames_data[total_best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(best_features_df, class_labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_estimators=1000, random_state=0)\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "r_squared_value = metrics.r2_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
